{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKVAttention(torch.nn.Module):\n",
    "    def __init__(self, scale, n, q_channels):\n",
    "        super().__init__()\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.scale = scale\n",
    "        self.n = n\n",
    "        self.q_channels = q_channels\n",
    "    \n",
    "    def forward(self, q, k, v): # Note: KINDA SUS, TRY TO GET RIGHT COMBO OF RESHAPE AND PERMUTE TO GET PROPER HEAD AND INDEX DIM SEPARATION\n",
    "        #following permutes are sussy, intention: to get the heads together but each index dim seperate\n",
    "        q = q.permute([0, 2, 1, 3])\n",
    "        k = k.permute([0, 2, 1, 3])\n",
    "        v = v.permute([0, 2, 1, 3])\n",
    "        att_factor = torch.matmul(q, k.permute([0, 1, 3, 2]))\n",
    "        att_factor = self.softmax(att_factor * self.scale)\n",
    "        attention = torch.matmul(att_factor, v)\n",
    "        attention = attention.permute([0, 2, 1, 3]) # intent: keep index dim together when heads are merged\n",
    "        attention = attention.reshape([-1, self.n, self.q_channels])\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = QKVAttention(0.5, n=2, q_channels=6)\n",
    "q = torch.randn([1, 2, 3, 2])\n",
    "k = torch.randn([1, 10, 3, 2])\n",
    "v = torch.randn([1, 10, 3, 2])\n",
    "attn(q, k, v).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, num_channels, wide_factor):\n",
    "        super().__init__()\n",
    "        inner_channels = wide_factor * num_channels\n",
    "        self.lin_in = torch.nn.Linear(num_channels, inner_channels) #Note: remove bias if needed\n",
    "        self.lin_out = torch.nn.Linear(inner_channels, num_channels) #Note: remove bias if needed\n",
    "        self.gelu = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin_in(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.lin_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_val = torch.randn([10, 32])\n",
    "mlp = MLP(32, 4)\n",
    "mlp(in_val).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, num_heads, n, q_channels, kv_channels):\n",
    "        super().__init__()\n",
    "        if q_channels % num_heads != 0:\n",
    "            raise ValueError('Bro, your damn channels don\\'t work with your head count!', q_channels, num_heads)\n",
    "        # if kv_channels % num_heads != 0:\n",
    "        #     raise ValueError('Bro, your damn channels don\\'t work with your head count!', kv_channels, num_heads)\n",
    "        self.num_heads = num_heads\n",
    "        self.q_head_channels = q_channels // num_heads\n",
    "        \n",
    "        self.q_trans = torch.nn.Linear(q_channels, q_channels)\n",
    "        self.k_trans = torch.nn.Linear(kv_channels, q_channels)\n",
    "        self.v_trans = torch.nn.Linear(kv_channels, q_channels)\n",
    "        self.out_trans = torch.nn.Linear(q_channels, q_channels)\n",
    "        self.qkvAttn = QKVAttention(q_channels ** -0.5, n, q_channels)\n",
    "\n",
    "    def forward(self, q_in, k_in, v_in):\n",
    "        q = self.q_trans(q_in)\n",
    "        k = self.k_trans(k_in)\n",
    "        v = self.v_trans(v_in)\n",
    "\n",
    "        q = q.reshape([-1, q.shape[1], self.num_heads, self.q_head_channels])\n",
    "        k = k.reshape([-1, k.shape[1], self.num_heads, self.q_head_channels])\n",
    "        v = v.reshape([-1, v.shape[1], self.num_heads, self.q_head_channels])\n",
    "\n",
    "        attn = self.qkvAttn(q, k, v)\n",
    "\n",
    "        out = self.out_trans(attn)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, n, num_channels, heads, wide_factor, p_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_norm = torch.nn.LayerNorm(num_channels)\n",
    "        self.mlp_norm = torch.nn.LayerNorm(num_channels)\n",
    "        self.attn = Attention(heads, n, num_channels, num_channels)\n",
    "        self.mlp = MLP(num_channels, wide_factor)\n",
    "        self.dropout = torch.nn.Dropout(p=p_dropout)\n",
    "\n",
    "    def forward(self, in_val):\n",
    "        x = in_val\n",
    "        attn = self.in_norm(in_val)\n",
    "        attn = self.attn(attn, attn, attn)\n",
    "        x += attn\n",
    "\n",
    "        mlp = self.mlp_norm(x)\n",
    "        mlp = self.mlp(mlp)\n",
    "        x += mlp\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfattn = SelfAttention(3, 4, 2, 4)\n",
    "garbo = torch.randn([1, 3, 4])\n",
    "selfattn(garbo).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(torch.nn.Module):\n",
    "    def __init__(self, n, q_channels, kv_channels, heads, wide_factor, p_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.q_norm = torch.nn.LayerNorm(q_channels)\n",
    "        self.kv_norm = torch.nn.LayerNorm(kv_channels)\n",
    "        self.mlp_norm = torch.nn.LayerNorm(q_channels)\n",
    "        self.attn = Attention(heads, n, q_channels, kv_channels)\n",
    "        self.mlp = MLP(q_channels, wide_factor)\n",
    "        self.dropout = torch.nn.Dropout(p=p_dropout)\n",
    "\n",
    "    def forward(self, q_kv):\n",
    "        q, kv = q_kv\n",
    "        x = q\n",
    "        q = self.q_norm(q)\n",
    "        kv = self.kv_norm(kv)\n",
    "        attn = self.attn(q, kv, kv)\n",
    "        x += attn\n",
    "\n",
    "        mlp = self.mlp_norm(x)\n",
    "        mlp = self.mlp(mlp)\n",
    "        x += mlp\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 4])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.randn([1, 5, 4])\n",
    "kv = torch.randn([1, 10, 20])\n",
    "cross = CrossAttention(5, 4, 20, 4, 4)\n",
    "cross((q, kv)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverEncoder(torch.nn.Module):\n",
    "    def __init__(self, n, q_channels, kv_channels, heads, wide_factor, latent_count, repeat_count=1, p_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.repeat_count = repeat_count\n",
    "\n",
    "        latentBlocks = [SelfAttention(n, q_channels, heads, wide_factor, p_dropout) for _ in range(latent_count)]\n",
    "        self.block = torch.nn.Sequential(\n",
    "            CrossAttention(n, q_channels, kv_channels, heads, wide_factor, p_dropout),\n",
    "            *latentBlocks\n",
    "        )\n",
    "    \n",
    "    def forward(self, q, kv):\n",
    "        x = self.block((q, kv))\n",
    "        for _ in range(self.repeat_count-1):\n",
    "            x = self.block((x, kv))\n",
    "        return self.block((q, kv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 10, 32])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = PerceiverEncoder(n=10, q_channels=32, kv_channels=64, heads=4, wide_factor=4, latent_count=6, repeat_count=5)\n",
    "q = torch.randn([6, 10, 32])\n",
    "kv = torch.randn([6, 20, 64])\n",
    "encoder(q, kv).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverInternal(torch.nn.Module):\n",
    "    def __init__(self, n, q_channels, kv_channels, heads, wide_factor, latent_count, q_out_dim, repeat_count=1, p_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = PerceiverEncoder(n, q_channels, kv_channels, heads, wide_factor, latent_count, repeat_count, p_dropout)\n",
    "\n",
    "        q_out = torch.zeros(q_out_dim)\n",
    "        torch.nn.init.xavier_normal_(q_out)\n",
    "        self.q_out = torch.nn.Parameter(q_out)\n",
    "\n",
    "        self.out_cross = CrossAttention(n, q_channels=q_out_dim[0], kv_channels=q_channels, heads=heads, wide_factor=wide_factor, p_dropout=p_dropout)\n",
    "\n",
    "    def forward(self, in_val):\n",
    "        x = self.encoder(in_val)\n",
    "        x = self.out_cross(self.q_out, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percInternal = PerceiverInternal(n=10, q_channels=32, kv_channels=64, heads=4, wide_factor=4, latent_count=3, q_out_dim=(37,42)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bba7e75823fc7474598fdaa9856f2217ab6bebf37b5fa259dabfeda760a6edd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('maintorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
